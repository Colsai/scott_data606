{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "HHSOIG_WP_Scraper_final.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyP2dTycAURRBNRFtt1MteaX",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Colsai/scott_data606/blob/main/Scrapers/HHSOIG_WP_Scraper_final.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# OIG Workplan & Reports Scraper (Step 0)\n",
        "## Data Scraper for HHS data and Reports\n",
        "To prepare for topic modeling, we need to scrape OIG's public website for its text data on current Audits and Evaluations. We'll use:\n",
        "https://oig.hhs.gov/reports-and-publications/workplan/\n",
        "\n",
        "- Scrape the summary pages of all work plan items\n",
        "https://oig.hhs.gov/reports-and-publications/workplan/summary/wp-summary-0000668.asp\n",
        "- Scrape the reports pages of all work plan items\n",
        "https://oig.hhs.gov/oei/reports/OEI-02-22-00310.asp"
      ],
      "metadata": {
        "id": "suWdtHP33TuB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "e74h_Pvstr4s",
        "outputId": "1d86f0e9-8449-47d8-96b0-b141b82a3fce"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "hgEWlTxJtnlW"
      },
      "outputs": [],
      "source": [
        "########################################\n",
        "# Import general packages for analysis #\n",
        "########################################\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import os\n",
        "import time\n",
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "from timeit import default_timer as timer\n",
        "\n",
        "#Scrape Active Table (last row is skipped- double header col)\n",
        "df_active_table = pd.read_html('https://oig.hhs.gov/reports-and-publications/workplan/active-item-table.asp')[0][0:-1]\n",
        "\n",
        "###############################\n",
        "# Summary Scraper             #\n",
        "###############################\n",
        "def scrape_all_summaries(last_scraped_page = 750, \n",
        "                         show_output = False):\n",
        "    '''\n",
        "    This function scrapes the summaries of the web pages\n",
        "    last_scraped_page is defined as the number at the end of the summary website:\n",
        "    https://oig.hhs.gov/reports-and-publications/workplan/summary/wp-summary-0000XXX.asp\n",
        "    '''\n",
        "    \n",
        "    df_all_workplans = []\n",
        "\n",
        "    for summary_num in range(1,last_scraped_page):\n",
        "        #Scrape sumamry number and fills to len(7) with 0s\n",
        "        strng_sum_num = str(summary_num)\n",
        "        \n",
        "        summ_num = strng_sum_num.zfill(7)\n",
        "\n",
        "        try:\n",
        "            workplan_website = f\"https://oig.hhs.gov/reports-and-publications/workplan/summary/wp-summary-{summ_num}.asp\"\n",
        "            df = pd.read_html(workplan_website)[0]\n",
        "            df[\"Website_Link\"] = workplan_website\n",
        "\n",
        "            if show_output:\n",
        "                print(summ_num)\n",
        "            \n",
        "            try:\n",
        "                #Scrape work plan website with bs4\n",
        "                response = requests.get(workplan_website)\n",
        "                \n",
        "                soup = BeautifulSoup(response.text, 'html.parser')\n",
        "\n",
        "                num_para_elements = len(soup.find_all('p'))\n",
        "\n",
        "                wp_summary = ''.join(str(soup.find_all('p')[3:num_para_elements])).replace(\"<p>\", \"\").replace(\"</p>\",\"\")[1:-1]\n",
        "\n",
        "                df[\"Summary\"] = wp_summary\n",
        "\n",
        "            except Exception as e:\n",
        "                print(e)\n",
        "                df[\"Summary\"] = \"\"\n",
        "                continue\n",
        "\n",
        "            #Append df to list    \n",
        "            df_all_workplans.append(df)\n",
        "        \n",
        "        except Exception as e:\n",
        "            if show_output:\n",
        "                print(f\"{summ_num}: {e}\")\n",
        "        \n",
        "            continue\n",
        "        \n",
        "    return pd.concat(df_all_workplans)\n",
        "\n",
        "def run_scraper(last_scraped_page = 750, show_output = True, output_file = False):\n",
        "    start = timer()\n",
        "    df_all_workplans = scrape_all_summaries(last_scraped_page, show_output)\n",
        "    end = timer()\n",
        "    run_time = f\"{round((end - start)/60,2)} minutes\"\n",
        "    print(f\"Total Run Time: {run_time}\")\n",
        "\n",
        "    if output_file == True:\n",
        "        #Output to CSV\n",
        "        df_all_workplans.to_csv('/content/drive/MyDrive/DATA_606/HHS_OIG_workplans.csv', index = False)\n",
        "\n",
        "    return df_all_workplans\n",
        "\n",
        "def scrape_all_reports(df_workplans, file_output = True):\n",
        "    #String Replacement\n",
        "    workplan_ids = [elem.replace(\" \", \"\").replace(\",\",\";\") if type(elem) == str \n",
        "                    else elem for elem in df_workplans['Report Number(s)']]\n",
        "\n",
        "    #Convert to dict\n",
        "    wp_list = [elem for idx, elem in enumerate(workplan_ids)]\n",
        "\n",
        "    #String Replacement\n",
        "    all_wps = [elem.split(\";\") if type(elem) == str else elem for elem in wp_list]\n",
        "    all_wps[0:10]\n",
        "\n",
        "    #Set potential products as anything that has an 'A' or starts with 'OEI\n",
        "    all_products = []\n",
        "    potential_products = []\n",
        "\n",
        "    for wp_list in all_wps:\n",
        "        wp_item_reports = []\n",
        "        if type(wp_list) == list:\n",
        "            for item in wp_list:\n",
        "                if item.startswith('A') | item.startswith('OEI'):\n",
        "                    wp_item_reports.append(item)\n",
        "                    all_products.append(item)\n",
        "        else:\n",
        "            wp_item_reports = ''\n",
        "            \n",
        "        potential_products.append(wp_item_reports)\n",
        "\n",
        "    df_output['Potential_products'] = potential_products\n",
        "\n",
        "    #Define the list of possible regions here\n",
        "    oas_iter_list = list(pd.Series([int(prod[2:4]) for prod in all_products if prod.startswith('A-')]).unique())\n",
        "    oei_iter_list = list(pd.Series([prod[4:6] for prod in all_products if prod.startswith('OEI')]).unique())\n",
        "\n",
        "    wp_item_stats = []\n",
        "\n",
        "###############################\n",
        "# Products Scraper            #\n",
        "###############################\n",
        "    for idx, prod in enumerate(all_products):\n",
        "        if idx % 5 == 0:\n",
        "            print(idx, end = ' ')\n",
        "\n",
        "        #\"A\" products are audits\n",
        "        if prod.startswith('A'):\n",
        "            wp_item_title = ''\n",
        "            wp_item_summary = ''\n",
        "            \n",
        "            #OAS' reports are defined within their websites as:\n",
        "            wp_test_num = prod.replace('-','')[-8:]\n",
        "            region_num = int(prod[2:4])\n",
        "            OAS_prod_website = f\"https://oig.hhs.gov/oas/reports/region{region_num}/{wp_test_num}.asp\"\n",
        "            response = requests.get(OAS_prod_website)\n",
        "\n",
        "            #If the response is positive, scrape the page.    \n",
        "            if str(response) == '<Response [200]>':\n",
        "                soup = BeautifulSoup(response.text, 'html.parser')\n",
        "                wp_item_title = str(soup.find_all('title')[0]).replace('<title>','').replace('</title>','')\n",
        "                wp_item_summary = str(soup.find_all('p')[5::]).replace(\"[\",\"\").replace(\"]\",\"\").replace(\"<p>\",\"\").replace(\"</p>\",\"\")\n",
        "                time.sleep(1)\n",
        "            \n",
        "            #Append scraped item\n",
        "            wp_item_stats.append([wp_item_title,wp_item_summary])\n",
        "\n",
        "        #OEI products are Evaluations and Inspections\n",
        "        elif prod.startswith('OEI'):\n",
        "            wp_item_title = ''\n",
        "            wp_item_summary = ''\n",
        "            OEI_prod_website = f\"https://oig.hhs.gov/oei/reports/{prod}.asp\"\n",
        "            response = requests.get(OEI_prod_website)\n",
        "            \n",
        "            #If the response is positive, scrape the page.    \n",
        "            if str(response) == '<Response [200]>':\n",
        "                soup = BeautifulSoup(response.text, 'html.parser')\n",
        "                wp_item_title = str(soup.find_all('title')[0]).replace('<title>','').replace('</title>','')\n",
        "                wp_item_summary = str(soup.find_all('p')[5::]).replace(\"[\",\"\").replace(\"]\",\"\").replace(\"<p>\",\"\").replace(\"</p>\",\"\")\n",
        "                time.sleep(1)\n",
        "            \n",
        "            #Append scraped item\n",
        "            wp_item_stats.append([wp_item_title,wp_item_summary])\n",
        "\n",
        "    #After scraping, \n",
        "    df_prods = pd.DataFrame(wp_item_stats)\n",
        "    df_prods.columns = ['Title', 'Summary']\n",
        "\n",
        "    df_combined = pd.DataFrame(all_products)\n",
        "    df_combined['Title'] = df_prods[\"Title\"]\n",
        "    df_combined['Summary'] = df_prods[\"Summary\"]\n",
        "    df_combined.columns = [\"Report Number(s)\",\"Workplan_Title\",\"Workplan_Summary\"].reset_index(drop = True)\n",
        "\n",
        "    if file_output == True:\n",
        "        try:\n",
        "            df_combined.to_csv('/content/drive/MyDrive/DATA_606/HHS_OIG_Reports.csv')\n",
        "        except Exception as e:\n",
        "            print(e)\n",
        "\n",
        "    return df_combined\n",
        "\n",
        "###############################\n",
        "# Start from Previous         #\n",
        "###############################\n",
        "def start_from_previous_wps():\n",
        "    df = pd.read_csv('/content/drive/MyDrive/DATA_606/HHS_OIG_workplans.csv')\n",
        "    return df\n",
        "\n",
        "def start_from_previous_products():\n",
        "    df = pd.read_csv('/content/drive/MyDrive/DATA_606/HHS_OIG_Reports.csv')\n",
        "    return df   "
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Save summaries"
      ],
      "metadata": {
        "id": "AF8c0lPCo8i3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df_workplan_summaries = scrape_all_summaries()"
      ],
      "metadata": {
        "id": "ExsjwQ4GhIoz"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Remove problem column here\n",
        "try:\n",
        "    df_workplan_summaries.drop(columns = 'Office of Evaluation and Inspections', inplace = True)\n",
        "except Exception as e:\n",
        "    print(e)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8EcmuKlFpa7T",
        "outputId": "cc2337e7-5284-4531-fe95-6cd3ee72e57e"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\"['Office of Evaluation and Inspections'] not found in axis\"\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df_workplan_summaries.to_csv('HHS_workplan_summaries.csv', index = False)"
      ],
      "metadata": {
        "id": "f9q9Y-QHiQus"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}